# RAG_based_QnA_chatbot
AI Document Q&amp;A Chatbot using LangChain and Hugging Face

ğŸ§  AI Document Q&A Chatbot (RAG Pipeline)
ğŸ’¬ Chat with your own PDFs using Generative AI + LangChain + Hugging Face



ğŸ§  Architecture (RAG Pipeline)

    PDF/Text Document â†’ Text Chunking â†’ Embeddings â†’ Vector Store
                                â†“
                           User Question
                                â†“
                     Retrieve Relevant Chunks
                                â†“
                      LLM Generates Answer


ğŸš€ Overview

Have you ever wished you could ask questions directly from your PDFs or text files â€” just like chatting with ChatGPT?
This project makes that possible using a Retrieval-Augmented Generation (RAG) pipeline!

It combines LangChain, Hugging Face Transformers, and Chroma Vector Database to build an intelligent chatbot that can read and answer questions from any uploaded document. ğŸ“„ğŸ¤–

ğŸ§© Key Features

âœ… Upload & Process PDFs or Text Files â€“ Read any document easily
âœ… Retrieval-Augmented Generation (RAG) â€“ Get context-aware answers from your own data
âœ… LLM-powered Answers â€“ Uses google/flan-t5-large from Hugging Face
âœ… Semantic Search with Embeddings â€“ Finds the most relevant parts of your document
âœ… Built-in Streamlit UI â€“ Simple, elegant interface for interactive chatting
âœ… Fully Local â€“ Works without sending your private data to external APIs

âš™ï¸ Tech Stack
Component	Description
LangChain	Framework for LLM orchestration & chaining
Hugging Face Transformers	Open-source LLM for text generation
ChromaDB	Vector database for efficient document retrieval
SentenceTransformers	For creating embeddings from text
Streamlit	Web interface for the chatbot
Python	Core language powering the backend
ğŸ§  How RAG Works

RAG = Retrieval + Generation

Load Document â†’ Read PDF or text

Chunk Text â†’ Split into small sections

Embed Text â†’ Convert each section into a vector (numerical meaning)

Store in Vector DB â†’ Save in Chroma for semantic search

Retrieve Context â†’ Find chunks relevant to userâ€™s query

Generate Answer â†’ LLM uses both retrieved context + query to answer

ğŸ§© Result: An LLM that understands your custom data â€” not just what it was trained on!

ğŸ’» How to Run the Project
1ï¸âƒ£ Clone this Repository
git clone https://github.com/your-username/rag-qa-chatbot.git
cd rag-qa-chatbot

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run the Streamlit App
streamlit run app.py

4ï¸âƒ£ Upload and Ask!

Upload any PDF or text file, then ask:

â€œWhat is this document about?â€
â€œWho is the author?â€
â€œSummarize the key points.â€

ğŸ“˜ Example

Try it with the sample document:
ğŸ“„ sample_ai_intro.pdf â€” Introduction to Artificial Intelligence

You can ask:

â€œWhat are the types of AI?â€
â€œWhat are the limitations of AI?â€

ğŸŒŸ Future Enhancements

ğŸ§± Add chat history + memory

ğŸ” Support multiple documents

â˜ï¸ Deploy on Hugging Face Spaces / Streamlit Cloud

ğŸ§  Add OpenAI or Gemini API for better generation quality

ğŸ’¾ Persistent storage for uploaded document embeddings

ğŸ§‘â€ğŸ’» Author

ğŸ‘‹ [Your Name]
Generative AI & LLM Enthusiast | Learning from Krish Naikâ€™s Complete GenAI Course

â€œBuilding smart apps that truly understand your data.â€
